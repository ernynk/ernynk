{
  "1": {
    "inputs": {
      "ckpt_name": "dreamshaper_8.safetensors",
      "beta_schedule": "sqrt_linear (AnimateDiff)",
      "use_custom_scale_factor": false,
      "scale_factor": 0.18215
    },
    "class_type": "CheckpointLoaderSimpleWithNoiseSelect",
    "_meta": {
      "title": "Load Checkpoint w/ Noise Select ğŸ­ğŸ…ğŸ…“"
    }
  },
  "2": {
    "inputs": {
      "vae_name": "vae-ft-mse-840000-ema-pruned.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "6": {
    "inputs": {
      "text": "(bad quality, worst quality:1.2)",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "7": {
    "inputs": {
      "seed": 44444444,
      "steps": 40,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "93",
        0
      ],
      "positive": [
        "107",
        0
      ],
      "negative": [
        "6",
        0
      ],
      "latent_image": [
        "101",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "93": {
    "inputs": {
      "model_name": "mm_sd_v15_v2.ckpt",
      "beta_schedule": "sqrt_linear (AnimateDiff)",
      "motion_scale": 1,
      "apply_v2_models_properly": true,
      "model": [
        "1",
        0
      ],
      "context_options": [
        "94",
        0
      ]
    },
    "class_type": "ADE_AnimateDiffLoaderWithContext",
    "_meta": {
      "title": "AnimateDiff Loader [Legacy] ğŸ­ğŸ…ğŸ…“â‘ "
    }
  },
  "94": {
    "inputs": {
      "context_length": 16,
      "context_stride": 1,
      "context_overlap": 4,
      "context_schedule": "uniform",
      "closed_loop": false,
      "fuse_method": "flat",
      "use_on_equal_length": false,
      "start_percent": 0,
      "guarantee_steps": 1
    },
    "class_type": "ADE_AnimateDiffUniformContextOptions",
    "_meta": {
      "title": "Context Optionsâ—†Looped Uniform ğŸ­ğŸ…ğŸ…“"
    }
  },
  "101": {
    "inputs": {
      "width": 768,
      "height": 768,
      "batch_size": 100
    },
    "class_type": "ADE_EmptyLatentImageLarge",
    "_meta": {
      "title": "Empty Latent Image (Big Batch) ğŸ­ğŸ…ğŸ…“"
    }
  },
  "102": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "scale_by": 1.5,
      "samples": [
        "7",
        0
      ]
    },
    "class_type": "LatentUpscaleBy",
    "_meta": {
      "title": "Upscale Latent By"
    }
  },
  "103": {
    "inputs": {
      "seed": 44444444,
      "steps": 25,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "93",
        0
      ],
      "positive": [
        "107",
        0
      ],
      "negative": [
        "6",
        0
      ],
      "latent_image": [
        "102",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler (upscale)"
    }
  },
  "104": {
    "inputs": {
      "samples": [
        "103",
        0
      ],
      "vae": [
        "2",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "107": {
    "inputs": {
      "text": "woman walking, red sundress, sunny day, warm weather, (good quality, best quality:1.2)",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "108": {
    "inputs": {
      "frame_rate": 24,
      "loop_count": 0,
      "filename_prefix": "AnimateDiff",
      "format": "image/webp",
      "pingpong": false,
      "save_image": true,
      "images": [
        "104",
        0
      ]
    },
    "class_type": "ADE_AnimateDiffCombine",
    "_meta": {
      "title": "ğŸš«AnimateDiff Combine [DEPRECATED, Use Video Combine (VHS) Instead!] ğŸ­ğŸ…ğŸ…“"
    }
  }
}
